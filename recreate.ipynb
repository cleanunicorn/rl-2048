{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f879fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "830f7ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(torch.__version__)\n",
    "# print(torch.backends.mps.is_available())\n",
    "# print(torch.tensor([1,2,3], device='mps'))  # Should succeed on Apple Silicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37ca476d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direction(Enum):\n",
    "    LEFT = 0\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8074fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Game2048Env:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 4\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "        self.spawn_tile()\n",
    "        self.spawn_tile()\n",
    "        self.score = 0\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def spawn_tile(self):\n",
    "        empty = list(zip(*np.where(self.board == 0)))\n",
    "        if empty:\n",
    "            x, y = empty[np.random.randint(len(empty))]\n",
    "            self.board[x, y] = 2 if np.random.random() < 0.9 else 4\n",
    "        \n",
    "    def step(self, action: Direction):\n",
    "        moved, reward = self.move(action.value)\n",
    "        if moved:\n",
    "            self.spawn_tile()\n",
    "        else:\n",
    "            # Stop if invalid move\n",
    "            return self.board.copy(), reward, True, {}\n",
    "        done = not self.can_move()\n",
    "        self.score += reward\n",
    "        return self.board.copy(), reward, done, {}\n",
    "    \n",
    "    def move(self, direction):\n",
    "        board = np.copy(self.board)\n",
    "        reward = 0\n",
    "        moved = False\n",
    "\n",
    "        # Rotate board so all moves are left-moves\n",
    "        for _ in range(direction):\n",
    "            board = np.rot90(board)\n",
    "            \n",
    "        for i in range(self.grid_size):\n",
    "            tiles = board[i][board[i] != 0]  # Extract non-zero\n",
    "            merged = []\n",
    "            j = 0\n",
    "            while j < len(tiles):\n",
    "                if j + 1 < len(tiles) and tiles[j] == tiles[j + 1]:\n",
    "                    merged_val = tiles[j] * 2\n",
    "                    reward += merged_val\n",
    "                    merged.append(merged_val)\n",
    "                    j += 2  # Skip next\n",
    "                    moved = True\n",
    "                else:\n",
    "                    merged.append(tiles[j])\n",
    "                    j += 1\n",
    "            # Pad with zeros to the right\n",
    "            merged += [0] * (self.grid_size - len(merged))\n",
    "            # Detect if move or merge happened\n",
    "            if not np.array_equal(board[i], merged):\n",
    "                moved = True\n",
    "            board[i] = merged\n",
    "\n",
    "        # Restore original orientation\n",
    "        for _ in range((4 - direction) % 4):\n",
    "            board = np.rot90(board)\n",
    "            \n",
    "        if moved:\n",
    "            self.board = board\n",
    "\n",
    "        return moved, reward\n",
    "\n",
    "    \n",
    "    def can_move(self):\n",
    "        for direction in range(4):\n",
    "            temp_board = self.board.copy()\n",
    "            moved, _ = self.move(direction)\n",
    "            self.board = temp_board  # Restore original\n",
    "            if moved:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6d8b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 0\n",
      "Reward: 0 | Done: False\n",
      "   2\t   0\t   0\t   2\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 4\n",
      "Reward: 4 | Done: False\n",
      "   4\t   0\t   0\t   2\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 4\n",
      "Reward: 0 | Done: False\n",
      "   4\t   2\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   2\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 4\n",
      "Reward: 0 | Done: False\n",
      "   4\t   2\t   0\t   0\n",
      "   2\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 8\n",
      "Reward: 4 | Done: False\n",
      "   4\t   2\t   2\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 12\n",
      "Reward: 4 | Done: False\n",
      "   4\t   4\t   0\t   0\n",
      "   4\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 20\n",
      "Reward: 8 | Done: False\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   2\t   2\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 24\n",
      "Reward: 4 | Done: False\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   4\t   0\t   0\n",
      "   0\t   0\t   4\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 32\n",
      "Reward: 8 | Done: False\n",
      "   8\t   0\t   0\t   4\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 32\n",
      "Reward: 0 | Done: False\n",
      "   8\t   4\t   0\t   0\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   2\t   2\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 36\n",
      "Reward: 4 | Done: False\n",
      "   8\t   4\t   0\t   0\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   4\t   2\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 36\n",
      "Reward: 0 | Done: True\n",
      "   8\t   4\t   0\t   0\n",
      "   8\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   4\t   2\t   0\t   0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "game = Game2048Env()\n",
    "state = game.reset()\n",
    "done = False\n",
    "\n",
    "def print_board(board):\n",
    "    for x in board:\n",
    "        print(\"\\t\".join(f\"{v:4}\" for v in x))\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print_board(state)\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:  # Play some random moves\n",
    "\n",
    "    # action = Direction(np.random.randint(4))  # Random action for demonstration\n",
    "    action = Direction.LEFT  # Fixed action for demonstration\n",
    "    state, reward, done, _ = game.step(action)\n",
    "\n",
    "    print(f\"Action: {action.name} | Score: {game.score}\")\n",
    "    print(f\"Reward: {reward} | Done: {done}\")\n",
    "    \n",
    "    print_board(state)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11794321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Determine the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "DEVICE = get_device()\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network using PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int = 16, hidden_layers: List[int] = [256], output_size: int = 4, empty: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if empty:\n",
    "            return\n",
    "        \n",
    "        # Build layers using PyTorch modules\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.Tanh())\n",
    "            prev_size = hidden_size\n",
    "            \n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using He initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(DEVICE)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity='tanh')\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Convert numpy array to tensor if needed and move to device\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float().to(DEVICE)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            x = x.to(DEVICE)\n",
    "        \n",
    "        return self.network(x)\n",
    "    \n",
    "    def mutate(self, mutation_rate: float = 0.1, mutation_strength: float = 0.5):\n",
    "        \"\"\"Mutate the network's weights and biases\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if torch.rand(1).item() < mutation_rate:\n",
    "                    mutation = torch.randn_like(param) * mutation_strength\n",
    "                    param.add_(mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dd0c9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameResult:\n",
    "    score: int\n",
    "    max_tile: int\n",
    "    moves: int\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, network: SimpleNeuralNetwork):\n",
    "        self.network = network\n",
    "\n",
    "    def play(self, env: Game2048Env, max_steps: int = 100) -> GameResult:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = self.next_move(state)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        return GameResult(score=total_reward, max_tile=np.max(state), moves=steps)\n",
    "    \n",
    "    def next_move(self, state: np.ndarray) -> Direction:\n",
    "        self.network.eval()  # Set to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            flat_state = state.flatten() / 2048.0  # Normalize input\n",
    "            q_values = self.network.forward(flat_state)\n",
    "            # Move back to CPU for numpy conversion\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            action = Direction(q_values_cpu.numpy().argmax())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1006ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvolutionaryOptimizer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            population_size: int = 50, \n",
    "            elite_size: int = 10,\n",
    "            mutation_rate: float = 0.1, \n",
    "            mutation_strength: float = 0.5,\n",
    "            hidden_layers: List[int] = [32]\n",
    "        ):\n",
    "        self.population_size = population_size\n",
    "        self.elite_size = elite_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.mutation_strength = mutation_strength\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Create initial population\n",
    "        self.population = []\n",
    "        for _ in range(population_size):\n",
    "            network = SimpleNeuralNetwork(hidden_layers=hidden_layers)\n",
    "            # Move to MPS if available\n",
    "            if torch.backends.mps.is_available():\n",
    "                network = network.to('mps')\n",
    "            self.population.append(network)\n",
    "\n",
    "    def evaluate(self, env: Game2048Env, games_per_player: int = 5, max_steps: int = 100) -> List[Tuple[SimpleNeuralNetwork, float]]:\n",
    "        results = []\n",
    "        for network in self.population:\n",
    "            player = Player(network)\n",
    "            total_score = 0\n",
    "            for _ in range(games_per_player):\n",
    "                game_result = player.play(env, max_steps=max_steps)\n",
    "                total_score += game_result.score\n",
    "            avg_score = total_score / games_per_player\n",
    "            results.append((network, avg_score))\n",
    "        return results\n",
    "\n",
    "    def select_and_breed(self, evaluated: List[Tuple[SimpleNeuralNetwork, float]]) -> None:\n",
    "        # Sort by score descending\n",
    "        evaluated.sort(key=lambda x: x[1], reverse=True)\n",
    "        elite = evaluated[:self.elite_size] \n",
    "\n",
    "        new_population = []\n",
    "        # Keep elite networks\n",
    "        for net, _ in elite:\n",
    "            new_population.append(net)\n",
    "        \n",
    "        # Create offspring by mutating elite networks\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent = random.choice(elite)[0]\n",
    "            \n",
    "            # Create a child by copying the parent's state\n",
    "            child = SimpleNeuralNetwork(hidden_layers=self.hidden_layers)\n",
    "            child.load_state_dict(parent.state_dict())\n",
    "            \n",
    "            # Move to same device as parent\n",
    "            child = child.to(next(parent.parameters()).device)\n",
    "            \n",
    "            # Mutate the child\n",
    "            child.mutate(self.mutation_rate, self.mutation_strength)\n",
    "            new_population.append(child)\n",
    "\n",
    "        self.population = new_population[:self.population_size]\n",
    "\n",
    "    def run_generation(self, env: Game2048Env, games_per_player: int = 5, max_steps: int = 1000) -> float:\n",
    "        evaluated = self.evaluate(env, games_per_player, max_steps=max_steps)\n",
    "        avg_score = sum(score for _, score in evaluated) / len(evaluated)\n",
    "        self.select_and_breed(evaluated)\n",
    "        return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc6676ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generation 1/1000 ===\n",
      "⏳ 2:25:56.830959 | Generation 1/1000 - Average Score: 26.783999999999995\n",
      "=== Generation 2/1000 ===\n",
      "⏳ 2:28:22.691603 | Generation 2/1000 - Average Score: 31.668000000000003\n",
      "=== Generation 3/1000 ===\n",
      "⏳ 2:24:54.953017 | Generation 3/1000 - Average Score: 34.64799999999999\n",
      "=== Generation 4/1000 ===\n",
      "⏳ 2:26:17.055721 | Generation 4/1000 - Average Score: 40.28399999999999\n",
      "=== Generation 5/1000 ===\n",
      "⏳ 2:26:45.799530 | Generation 5/1000 - Average Score: 38.228\n",
      "=== Generation 6/1000 ===\n",
      "⏳ 2:28:49.647298 | Generation 6/1000 - Average Score: 42.25200000000001\n",
      "=== Generation 7/1000 ===\n",
      "⏳ 2:32:51.602964 | Generation 7/1000 - Average Score: 49.744000000000014\n",
      "=== Generation 8/1000 ===\n",
      "⏳ 2:40:56.839488 | Generation 8/1000 - Average Score: 51.176\n",
      "=== Generation 9/1000 ===\n",
      "⏳ 2:45:59.362586 | Generation 9/1000 - Average Score: 53.95600000000004\n",
      "=== Generation 10/1000 ===\n",
      "⏳ 2:50:08.490770 | Generation 10/1000 - Average Score: 52.2\n",
      "=== Generation 11/1000 ===\n",
      "⏳ 2:55:33.861284 | Generation 11/1000 - Average Score: 53.596\n",
      "=== Generation 12/1000 ===\n",
      "⏳ 2:56:48.534257 | Generation 12/1000 - Average Score: 56.88799999999998\n",
      "=== Generation 13/1000 ===\n",
      "⏳ 2:58:38.097380 | Generation 13/1000 - Average Score: 61.435999999999986\n",
      "=== Generation 14/1000 ===\n",
      "⏳ 3:04:32.823908 | Generation 14/1000 - Average Score: 68.28400000000002\n",
      "=== Generation 15/1000 ===\n",
      "⏳ 3:09:27.800804 | Generation 15/1000 - Average Score: 67.10000000000002\n",
      "=== Generation 16/1000 ===\n",
      "⏳ 3:17:13.038498 | Generation 16/1000 - Average Score: 81.28400000000002\n",
      "=== Generation 17/1000 ===\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 46\u001b[39m\n\u001b[32m     43\u001b[39m     plt.show()\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m gen \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(generations):\n\u001b[32m     24\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m=== Generation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgen+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgenerations\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     avg_score = \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_generation\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames_per_player\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m     avg_scores.append(avg_score)\n\u001b[32m     28\u001b[39m     elapsed_time = time.time() - loop_start_time\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mEvolutionaryOptimizer.run_generation\u001b[39m\u001b[34m(self, env, games_per_player, max_steps)\u001b[39m\n\u001b[32m     64\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_generation\u001b[39m(\u001b[38;5;28mself\u001b[39m, env: Game2048Env, games_per_player: \u001b[38;5;28mint\u001b[39m = \u001b[32m5\u001b[39m, max_steps: \u001b[38;5;28mint\u001b[39m = \u001b[32m1000\u001b[39m) -> \u001b[38;5;28mfloat\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     evaluated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgames_per_player\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m     avg_score = \u001b[38;5;28msum\u001b[39m(score \u001b[38;5;28;01mfor\u001b[39;00m _, score \u001b[38;5;129;01min\u001b[39;00m evaluated) / \u001b[38;5;28mlen\u001b[39m(evaluated)\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m.select_and_breed(evaluated)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mEvolutionaryOptimizer.evaluate\u001b[39m\u001b[34m(self, env, games_per_player, max_steps)\u001b[39m\n\u001b[32m     29\u001b[39m total_score = \u001b[32m0\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(games_per_player):\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     game_result = \u001b[43mplayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     total_score += game_result.score\n\u001b[32m     33\u001b[39m avg_score = total_score / games_per_player\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mPlayer.play\u001b[39m\u001b[34m(self, env, max_steps)\u001b[39m\n\u001b[32m     15\u001b[39m steps = \u001b[32m0\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done \u001b[38;5;129;01mand\u001b[39;00m steps < max_steps:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnext_move\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     state, reward, done, _ = env.step(action)\n\u001b[32m     21\u001b[39m     total_reward += reward\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mPlayer.next_move\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     29\u001b[39m     flat_state = state.flatten() / \u001b[32m2048.0\u001b[39m  \u001b[38;5;66;03m# Normalize input\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     q_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Move back to CPU for numpy conversion\u001b[39;00m\n\u001b[32m     32\u001b[39m     q_values_cpu = q_values.cpu()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mSimpleNeuralNetwork.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Convert numpy array to tensor if needed and move to device\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np.ndarray):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     x = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.to(DEVICE)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, torch.Tensor):\n\u001b[32m     60\u001b[39m     x = x.to(DEVICE)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "best_network = None\n",
    "best_score = 0\n",
    "\n",
    "from datetime import timedelta\n",
    "\n",
    "def main():\n",
    "    env = Game2048Env()\n",
    "    optimizer = EvolutionaryOptimizer(\n",
    "        population_size=100, \n",
    "        elite_size=50, \n",
    "        mutation_rate=0.1, \n",
    "        mutation_strength=0.3, \n",
    "        hidden_layers=[512, 256, 128]\n",
    "    )\n",
    "    generations = 1000\n",
    "    games_per_player = 10\n",
    "    max_steps = 10000\n",
    "\n",
    "    avg_scores = []\n",
    "\n",
    "    loop_start_time = time.time()\n",
    "\n",
    "    for gen in range(generations):\n",
    "        print(f\"=== Generation {gen+1}/{generations} ===\")\n",
    "        \n",
    "        avg_score = optimizer.run_generation(env, games_per_player, max_steps)\n",
    "        avg_scores.append(avg_score)\n",
    "        elapsed_time = time.time() - loop_start_time\n",
    "        average_time_per_iteration = elapsed_time / (gen + 1)\n",
    "        duration = str(timedelta(seconds=(average_time_per_iteration * (generations - gen + 1))))\n",
    "        \n",
    "        print(f\"⏳ {duration} | Generation {gen+1}/{generations} - Average Score: {avg_score}\")\n",
    "\n",
    "    global best_network, best_score\n",
    "    evaluated = optimizer.evaluate(env, games_per_player)\n",
    "    best_network, best_score = max(evaluated, key=lambda x: x[1])\n",
    "\n",
    "    # Plot average scores over generations\n",
    "    plt.plot(range(1, generations + 1), avg_scores)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.title('Evolution of Average Score over Generations')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b20920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Played a game - Score: 72, Max Tile: 16, Moves: 19\n",
      "Played a game - Score: 124, Max Tile: 16, Moves: 31\n",
      "Played a game - Score: 84, Max Tile: 16, Moves: 23\n",
      "Played a game - Score: 64, Max Tile: 16, Moves: 14\n",
      "Played a game - Score: 52, Max Tile: 16, Moves: 13\n",
      "Played a game - Score: 112, Max Tile: 16, Moves: 29\n",
      "Played a game - Score: 184, Max Tile: 32, Moves: 37\n",
      "Played a game - Score: 12, Max Tile: 4, Moves: 7\n",
      "Played a game - Score: 160, Max Tile: 16, Moves: 35\n",
      "Played a game - Score: 28, Max Tile: 8, Moves: 17\n",
      "Played a game - Score: 64, Max Tile: 8, Moves: 26\n",
      "Played a game - Score: 148, Max Tile: 16, Moves: 34\n",
      "Played a game - Score: 24, Max Tile: 8, Moves: 13\n",
      "Played a game - Score: 36, Max Tile: 8, Moves: 15\n",
      "Played a game - Score: 24, Max Tile: 8, Moves: 10\n",
      "Played a game - Score: 240, Max Tile: 32, Moves: 41\n",
      "Played a game - Score: 84, Max Tile: 16, Moves: 24\n",
      "Played a game - Score: 120, Max Tile: 16, Moves: 28\n",
      "Played a game - Score: 4, Max Tile: 4, Moves: 4\n",
      "Played a game - Score: 120, Max Tile: 16, Moves: 30\n",
      "Played a game - Score: 44, Max Tile: 16, Moves: 9\n",
      "Played a game - Score: 44, Max Tile: 8, Moves: 21\n",
      "Played a game - Score: 28, Max Tile: 8, Moves: 13\n",
      "Played a game - Score: 48, Max Tile: 8, Moves: 19\n",
      "Played a game - Score: 32, Max Tile: 8, Moves: 13\n",
      "Played a game - Score: 68, Max Tile: 16, Moves: 18\n",
      "Played a game - Score: 124, Max Tile: 16, Moves: 31\n",
      "Played a game - Score: 104, Max Tile: 16, Moves: 25\n",
      "Played a game - Score: 108, Max Tile: 16, Moves: 24\n",
      "Played a game - Score: 32, Max Tile: 8, Moves: 14\n",
      "Played a game - Score: 152, Max Tile: 16, Moves: 31\n",
      "Played a game - Score: 80, Max Tile: 16, Moves: 20\n",
      "Played a game - Score: 64, Max Tile: 16, Moves: 18\n",
      "Played a game - Score: 76, Max Tile: 16, Moves: 20\n",
      "Played a game - Score: 68, Max Tile: 16, Moves: 17\n",
      "Played a game - Score: 60, Max Tile: 16, Moves: 15\n",
      "Played a game - Score: 136, Max Tile: 16, Moves: 33\n",
      "Played a game - Score: 20, Max Tile: 8, Moves: 6\n",
      "Played a game - Score: 64, Max Tile: 16, Moves: 15\n",
      "Played a game - Score: 88, Max Tile: 16, Moves: 23\n",
      "Played a game - Score: 52, Max Tile: 8, Moves: 19\n",
      "Played a game - Score: 28, Max Tile: 8, Moves: 13\n",
      "Played a game - Score: 76, Max Tile: 16, Moves: 22\n",
      "Played a game - Score: 40, Max Tile: 8, Moves: 15\n",
      "Played a game - Score: 8, Max Tile: 4, Moves: 5\n",
      "Played a game - Score: 100, Max Tile: 16, Moves: 30\n",
      "Played a game - Score: 212, Max Tile: 32, Moves: 40\n",
      "Played a game - Score: 180, Max Tile: 32, Moves: 31\n",
      "Played a game - Score: 132, Max Tile: 16, Moves: 34\n",
      "Played a game - Score: 4, Max Tile: 4, Moves: 3\n",
      "Played a game - Score: 108, Max Tile: 16, Moves: 34\n",
      "Played a game - Score: 64, Max Tile: 8, Moves: 28\n",
      "Played a game - Score: 148, Max Tile: 16, Moves: 34\n",
      "Played a game - Score: 68, Max Tile: 16, Moves: 16\n",
      "Played a game - Score: 24, Max Tile: 8, Moves: 8\n",
      "Played a game - Score: 84, Max Tile: 8, Moves: 29\n",
      "Played a game - Score: 112, Max Tile: 16, Moves: 27\n",
      "Played a game - Score: 116, Max Tile: 16, Moves: 30\n",
      "Played a game - Score: 68, Max Tile: 16, Moves: 17\n",
      "Played a game - Score: 148, Max Tile: 16, Moves: 39\n",
      "Played a game - Score: 28, Max Tile: 8, Moves: 14\n",
      "Played a game - Score: 64, Max Tile: 8, Moves: 23\n",
      "Played a game - Score: 20, Max Tile: 8, Moves: 7\n",
      "Played a game - Score: 116, Max Tile: 16, Moves: 28\n",
      "Played a game - Score: 0, Max Tile: 2, Moves: 2\n",
      "Played a game - Score: 72, Max Tile: 16, Moves: 18\n",
      "Played a game - Score: 164, Max Tile: 16, Moves: 42\n",
      "Played a game - Score: 12, Max Tile: 4, Moves: 11\n",
      "Played a game - Score: 236, Max Tile: 32, Moves: 46\n",
      "Played a game - Score: 4, Max Tile: 4, Moves: 4\n",
      "Played a game - Score: 76, Max Tile: 16, Moves: 20\n",
      "Played a game - Score: 64, Max Tile: 16, Moves: 17\n",
      "Played a game - Score: 48, Max Tile: 8, Moves: 22\n",
      "Played a game - Score: 0, Max Tile: 2, Moves: 2\n",
      "Played a game - Score: 0, Max Tile: 2, Moves: 2\n",
      "Played a game - Score: 32, Max Tile: 8, Moves: 11\n",
      "Played a game - Score: 200, Max Tile: 32, Moves: 36\n",
      "Played a game - Score: 156, Max Tile: 16, Moves: 37\n",
      "Played a game - Score: 68, Max Tile: 16, Moves: 16\n",
      "Played a game - Score: 152, Max Tile: 16, Moves: 34\n",
      "Played a game - Score: 48, Max Tile: 8, Moves: 16\n",
      "Played a game - Score: 128, Max Tile: 16, Moves: 30\n",
      "Played a game - Score: 164, Max Tile: 32, Moves: 28\n",
      "Played a game - Score: 92, Max Tile: 16, Moves: 25\n",
      "Played a game - Score: 160, Max Tile: 16, Moves: 38\n",
      "Played a game - Score: 8, Max Tile: 4, Moves: 5\n",
      "Played a game - Score: 64, Max Tile: 16, Moves: 14\n",
      "Played a game - Score: 36, Max Tile: 8, Moves: 11\n",
      "Played a game - Score: 96, Max Tile: 16, Moves: 25\n",
      "Played a game - Score: 88, Max Tile: 16, Moves: 27\n",
      "Played a game - Score: 0, Max Tile: 4, Moves: 4\n",
      "Played a game - Score: 112, Max Tile: 16, Moves: 25\n",
      "Played a game - Score: 36, Max Tile: 8, Moves: 19\n",
      "Played a game - Score: 232, Max Tile: 32, Moves: 41\n",
      "Played a game - Score: 0, Max Tile: 2, Moves: 2\n",
      "Played a game - Score: 152, Max Tile: 16, Moves: 39\n",
      "Played a game - Score: 0, Max Tile: 2, Moves: 2\n",
      "Played a game - Score: 160, Max Tile: 16, Moves: 41\n",
      "Played a game - Score: 120, Max Tile: 16, Moves: 28\n",
      "Played a game - Score: 4, Max Tile: 4, Moves: 3\n",
      "Best tile: 32, Best score: 240\n"
     ]
    }
   ],
   "source": [
    "# Playing with the best network\n",
    "if best_network:\n",
    "    best_tile = 0\n",
    "    best_score = 0\n",
    "    for _ in range(100):\n",
    "        env = Game2048Env()\n",
    "        player = Player(best_network)\n",
    "        result = player.play(env, max_steps=1000)\n",
    "        best_tile = result.max_tile if best_tile < result.max_tile else best_tile\n",
    "        best_score = result.score if best_score < result.score else best_score\n",
    "        print(f\"Played a game - Score: {result.score}, Max Tile: {result.max_tile}, Moves: {result.moves}\")\n",
    "    print(f\"Best tile: {best_tile}, Best score: {best_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5afac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   2\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT, Best: 4\n",
      "   0\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: UP, Best: 4\n",
      "   4\t   0\t   0\t   2\n",
      "   0\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: RIGHT, Best: 4\n",
      "   0\t   0\t   4\t   2\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 4\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   4\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   2\t   4\t   4\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   0\t   2\t   4\t   8\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   2\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   4\t   0\n",
      "   0\t   2\t   2\t   8\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   0\t   2\t   4\t   2\n",
      "   0\t   0\t   2\t   8\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   2\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   4\t   2\n",
      "   2\t   2\t   4\t   8\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   2\n",
      "   0\t   0\t   0\t   8\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   2\n",
      "   4\t   2\t   8\t   8\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   2\n",
      "   4\t   2\t   0\t   8\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   2\n",
      "   4\t   4\t   8\t   8\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   2\n",
      "   4\t   4\t   0\t   8\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   2\n",
      "   2\t   2\t   0\t   8\n",
      "   4\t   4\t   8\t   2\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   4\n",
      "   4\t   4\t   0\t   8\n",
      "   0\t   0\t   2\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   2\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   4\n",
      "   2\t   2\t   8\t   8\n",
      "   4\t   4\t   2\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   4\n",
      "   4\t   2\t   8\t   8\n",
      "   4\t   4\t   2\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   4\n",
      "   2\t   2\t   8\t   8\n",
      "   8\t   4\t   2\t   2\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   2\n",
      "   8\t   4\t   2\t   4\n",
      "   0\t   0\t   2\t   8\n",
      "   0\t   0\t   0\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   2\t   0\t   2\n",
      "   0\t   0\t   0\t   4\n",
      "   2\t   2\t   8\t   8\n",
      "   8\t   4\t   4\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   2\t   0\t   2\n",
      "   0\t   0\t   0\t   4\n",
      "   2\t   4\t   8\t   8\n",
      "   8\t   4\t   4\t   2\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   8\t   2\n",
      "   8\t   8\t   4\t   4\n",
      "   0\t   2\t   0\t   8\n",
      "   0\t   0\t   0\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   0\t   2\n",
      "   0\t   2\t   2\t   4\n",
      "   2\t   8\t   8\t   8\n",
      "   8\t   2\t   4\t   2\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   2\t   2\n",
      "   8\t   8\t   8\t   4\n",
      "   0\t   2\t   4\t   8\n",
      "   0\t   2\t   0\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   0\t   2\t   2\n",
      "   0\t   2\t   2\t   4\n",
      "   2\t   8\t   8\t   8\n",
      "   8\t   4\t   4\t   2\n",
      "--------------------\n",
      "Action: UP, Best: 8\n",
      "   2\t   2\t   4\t   2\n",
      "   8\t   8\t   8\t   4\n",
      "   0\t   4\t   4\t   8\n",
      "   0\t   2\t   0\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "   0\t   2\t   2\t   2\n",
      "   0\t   8\t   4\t   4\n",
      "   2\t   4\t   8\t   8\n",
      "   8\t   2\t   4\t   2\n",
      "--------------------\n",
      "Action: DOWN, Best: 8\n",
      "nop\n",
      "   0\t   2\t   2\t   2\n",
      "   0\t   8\t   4\t   4\n",
      "   2\t   4\t   8\t   8\n",
      "   8\t   2\t   4\t   2\n",
      "--------------------\n",
      "   0\t   2\t   2\t   2\n",
      "   0\t   8\t   4\t   4\n",
      "   2\t   4\t   8\t   8\n",
      "   8\t   2\t   4\t   2\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "if best_network:\n",
    "    env = Game2048Env()\n",
    "    player = Player(best_network)\n",
    "\n",
    "    while True:\n",
    "        board = env.board\n",
    "        print_board(board)\n",
    "        action = player.next_move(board)\n",
    "        prev_state = state.copy()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        print(f\"Action: {action.name}, Best: {np.max(state)}\")\n",
    "        if (prev_state == state).all():\n",
    "            print(\"nop\")\n",
    "            print_board(prev_state)\n",
    "            print_board(state)\n",
    "            break\n",
    "        if done:\n",
    "            print(\"Game Over\")\n",
    "            print_board(state)\n",
    "            break\n",
    "\n",
    "        #time.sleep(.2)  # Pause for a second to visualize\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927375fe-cb93-4ebf-ba18-b58d8c624b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: bestnetwork-1759940911.105819.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"bestnetwork-{time.time()}.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_network, f)\n",
    "    print(f\"Saved: {f.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cow-search (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
