{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f879fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import cupy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Dict\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "830f7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0+cu128\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.mps.is_available())\n",
    "print(torch.cpu.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8074fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Direction(Enum):\n",
    "    LEFT = 0\n",
    "    UP = 1\n",
    "    RIGHT = 2\n",
    "    DOWN = 3\n",
    "\n",
    "class Game2048Env:\n",
    "    def __init__(self):\n",
    "        self.grid_size = 4\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.board = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "        self.spawn_tile()\n",
    "        self.spawn_tile()\n",
    "        self.score = 0\n",
    "        return self.board.copy()\n",
    "    \n",
    "    def spawn_tile(self):\n",
    "        empty = list(zip(*np.where(self.board == 0)))\n",
    "        if empty:\n",
    "            x, y = empty[np.random.randint(len(empty))]\n",
    "            self.board[x, y] = 2 if np.random.random() < 0.9 else 4\n",
    "        \n",
    "    def step(self, action: Direction):\n",
    "        moved, reward = self.move(action.value)\n",
    "        if moved:\n",
    "            self.spawn_tile()\n",
    "        else:\n",
    "            # Stop if invalid move\n",
    "            return self.board.copy(), reward, True, {}\n",
    "        done = not self.can_move()\n",
    "        self.score += reward\n",
    "        return self.board.copy(), reward, done, {}\n",
    "    \n",
    "    def move(self, direction):\n",
    "        board = np.copy(self.board)\n",
    "        reward = 0\n",
    "        moved = False\n",
    "\n",
    "        # Rotate board so all moves are left-moves\n",
    "        for _ in range(direction):\n",
    "            board = np.rot90(board)\n",
    "            \n",
    "        for i in range(self.grid_size):\n",
    "            tiles = board[i][board[i] != 0]  # Extract non-zero\n",
    "            merged = []\n",
    "            j = 0\n",
    "            while j < len(tiles):\n",
    "                if j + 1 < len(tiles) and tiles[j] == tiles[j + 1]:\n",
    "                    merged_val = tiles[j] * 2\n",
    "                    reward += merged_val\n",
    "                    merged.append(merged_val)\n",
    "                    j += 2  # Skip next\n",
    "                    moved = True\n",
    "                else:\n",
    "                    merged.append(tiles[j])\n",
    "                    reward += 0\n",
    "                    j += 1\n",
    "            # Pad with zeros to the right\n",
    "            merged += [0] * (self.grid_size - len(merged))\n",
    "            # Detect if move or merge happened\n",
    "            if not np.array_equal(board[i], merged):\n",
    "                moved = True\n",
    "            board[i] = merged\n",
    "\n",
    "        # Restore original orientation\n",
    "        for _ in range((4 - direction) % 4):\n",
    "            board = np.rot90(board)\n",
    "            \n",
    "        if moved:\n",
    "            self.board = board\n",
    "\n",
    "        return moved, reward\n",
    "\n",
    "    \n",
    "    def can_move(self):\n",
    "        for direction in range(4):\n",
    "            temp_board = self.board.copy()\n",
    "            moved, _ = self.move(direction)\n",
    "            self.board = temp_board  # Restore original\n",
    "            if moved:\n",
    "                return True\n",
    "        return False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6d8b203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0\t   2\t   0\t   0\n",
      "   4\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 0\n",
      "Reward: 0 | Done: False\n",
      "   2\t   0\t   0\t   0\n",
      "   4\t   0\t   0\t   2\n",
      "   0\t   0\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 0\n",
      "Reward: 0 | Done: False\n",
      "   2\t   0\t   0\t   0\n",
      "   4\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n",
      "Action: LEFT | Score: 0\n",
      "Reward: 0 | Done: True\n",
      "   2\t   0\t   0\t   0\n",
      "   4\t   2\t   0\t   0\n",
      "   0\t   0\t   0\t   0\n",
      "   2\t   0\t   0\t   0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "game = Game2048Env()\n",
    "state = game.reset()\n",
    "done = False\n",
    "\n",
    "def print_board(board):\n",
    "    for x in board:\n",
    "        print(\"\\t\".join(f\"{v:4}\" for v in x))\n",
    "    print(\"-\" * 20)\n",
    "\n",
    "print_board(state)\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done:  # Play some random moves\n",
    "\n",
    "    # action = Direction(np.random.randint(4))  # Random action for demonstration\n",
    "    action = Direction.LEFT  # Fixed action for demonstration\n",
    "    state, reward, done, _ = game.step(action)\n",
    "\n",
    "    print(f\"Action: {action.name} | Score: {game.score}\")\n",
    "    print(f\"Reward: {reward} | Done: {done}\")\n",
    "    \n",
    "    print_board(state)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11794321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Determine the best available device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "\n",
    "# DEVICE = get_device()\n",
    "DEVICE = torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    \"\"\"Simple feedforward neural network using PyTorch\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int = 16, hidden_layers: List[int] = [256], output_size: int = 4, empty: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        if empty:\n",
    "            return\n",
    "        \n",
    "        # Build layers using PyTorch modules\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        # Add hidden layers\n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            # layers.append(nn.Tanh())\n",
    "            layers.append(nn.ReLU())\n",
    "            prev_size = hidden_size\n",
    "            \n",
    "        # Add output layer (no activation)\n",
    "        layers.append(nn.Linear(prev_size, output_size))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "        \n",
    "        # Initialize weights using He initialization\n",
    "        self._initialize_weights()\n",
    "        \n",
    "        # Move to device\n",
    "        self.to(DEVICE)\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize weights using He initialization\"\"\"\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal_(module.weight, nonlinearity='tanh')\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        # Convert numpy array to tensor if needed and move to device\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = torch.from_numpy(x).float().to(DEVICE)\n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            x = x.to(DEVICE)\n",
    "        \n",
    "        return self.network(x)\n",
    "    \n",
    "    def mutate(self, mutation_rate: float = 0.1, mutation_strength: float = 0.5):\n",
    "        \"\"\"Mutate the network's weights and biases\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                if torch.rand(1).item() < mutation_rate:\n",
    "                    mutation = torch.randn_like(param) * mutation_strength\n",
    "                    param.add_(mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd0c9cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class GameResult:\n",
    "    score: int\n",
    "    max_tile: int\n",
    "    moves: int\n",
    "\n",
    "class Player:\n",
    "    def __init__(self, network: SimpleNeuralNetwork):\n",
    "        self.network = network\n",
    "\n",
    "    def play(self, env: Game2048Env, max_steps: int = 100) -> GameResult:\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "\n",
    "        while not done and steps < max_steps:\n",
    "            action = self.next_move(state)\n",
    "\n",
    "            state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "        return GameResult(score=total_reward, max_tile=np.max(state), moves=steps)\n",
    "    \n",
    "    def next_move(self, state: np.ndarray) -> Direction:\n",
    "        self.network.eval()  # Set to evaluation mode\n",
    "        with torch.no_grad():\n",
    "            flat_state = state.flatten() / 2048.0  # Normalize input\n",
    "            q_values = self.network.forward(flat_state)\n",
    "            # Move back to CPU for numpy conversion\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            action = Direction(q_values_cpu.numpy().argmax())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1006ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import joblib\n",
    "\n",
    "class EvolutionaryOptimizer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            population_size: int = 50, \n",
    "            elite_size: int = 10,\n",
    "            mutation_rate: float = 0.1, \n",
    "            mutation_strength: float = 0.5,\n",
    "            hidden_layers: List[int] = [32]\n",
    "        ):\n",
    "        self.population_size = population_size\n",
    "        self.elite_size = elite_size\n",
    "        self.mutation_rate = mutation_rate\n",
    "        self.mutation_strength = mutation_strength\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Create initial population\n",
    "        self.population = []\n",
    "        for _ in range(population_size):\n",
    "            network = SimpleNeuralNetwork(hidden_layers=hidden_layers)\n",
    "            # Move to MPS if available\n",
    "            # if torch.backends.mps.is_available():\n",
    "            #     network = network.to('mps')\n",
    "            self.population.append(network)\n",
    "\n",
    "    def evaluate(self, env: Game2048Env, games_per_player: int = 5, max_steps: int = 100) -> List[Tuple[SimpleNeuralNetwork, float, int]]:\n",
    "        def eval_network(network):\n",
    "            # network_cpu = network.cpu()\n",
    "            player = Player(network)\n",
    "            env = Game2048Env()\n",
    "            total_score = 0\n",
    "            best_score = 0\n",
    "            for _ in range(games_per_player):\n",
    "                game_result = player.play(env, max_steps=max_steps)\n",
    "                total_score += game_result.score\n",
    "                if game_result.score > best_score:\n",
    "                    best_score = game_result.score\n",
    "            avg_score = total_score / games_per_player\n",
    "            return (network, avg_score, best_score)\n",
    "\n",
    "        results = Parallel(n_jobs=joblib.cpu_count(), prefer=\"threads\")(delayed(eval_network)(net) for net in self.population)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def select_and_breed(self, evaluated: List[Tuple[SimpleNeuralNetwork, float]]) -> None:\n",
    "        # Sort by score descending\n",
    "        evaluated.sort(key=lambda x: x[1], reverse=True)\n",
    "        elite = evaluated[:self.elite_size] \n",
    "\n",
    "        new_population = []\n",
    "        # Keep elite networks\n",
    "        for net, _, _ in elite:\n",
    "            new_population.append(net)\n",
    "        \n",
    "        # Create offspring by mutating elite networks\n",
    "        while len(new_population) < self.population_size:\n",
    "            parent = random.choice(elite)[0]\n",
    "            \n",
    "            # Create a child by copying the parent's state\n",
    "            child = SimpleNeuralNetwork(hidden_layers=self.hidden_layers)\n",
    "            child.load_state_dict(parent.state_dict())\n",
    "            \n",
    "            # Move to same device as parent\n",
    "            child = child.to(DEVICE)\n",
    "            \n",
    "            # Mutate the child\n",
    "            child.mutate(self.mutation_rate, self.mutation_strength)\n",
    "            new_population.append(child)\n",
    "\n",
    "        self.population = new_population[:self.population_size]\n",
    "\n",
    "    def run_generation(self, env: Game2048Env, games_per_player: int = 5, max_steps: int = 1000):\n",
    "        evaluated = self.evaluate(env, games_per_player, max_steps=max_steps)\n",
    "        avg_score = sum(score for _, score, _ in evaluated) / len(evaluated)\n",
    "        best_score = max(score for _, _, score in evaluated)\n",
    "        \n",
    "        self.select_and_breed(evaluated)\n",
    "        \n",
    "        return self.population, avg_score, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6d7bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_network(network: SimpleNeuralNetwork, filename: str):\n",
    "    torch.save(network.state_dict(), filename)\n",
    "\n",
    "def load_network(filename: str, hidden_layers: List[int]) -> SimpleNeuralNetwork:\n",
    "    network = SimpleNeuralNetwork(hidden_layers=hidden_layers)\n",
    "    network.load_state_dict(torch.load(filename, map_location=DEVICE))\n",
    "    network.to(DEVICE)\n",
    "    return network\n",
    "\n",
    "def save_population(population: List[SimpleNeuralNetwork], filename: str):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(population, f)   \n",
    "\n",
    "def load_population(filename: str) -> List[SimpleNeuralNetwork]:\n",
    "    with open(filename, 'rb') as f:\n",
    "        population = pickle.load(f)\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6676ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ 16:33:51.912201 | Generation 1/5000 - Average Score: 23.091999999999995 - Best Score: 232 \n",
      "Saving population at generation 1\n",
      "⏳ 20:55:22.717382 | Generation 11/5000 - Average Score: 51.736000000000004 - Best Score: 416 \n",
      "Saving population at generation 11\n",
      "⏳ 23:03:25.630531 | Generation 21/5000 - Average Score: 62.15999999999999 - Best Score: 288 \n",
      "Saving population at generation 21\n",
      "⏳ 1 day, 0:26:50.045582 | Generation 31/5000 - Average Score: 59.576000000000015 - Best Score: 304 \n",
      "Saving population at generation 31\n",
      "⏳ 1 day, 1:35:36.842905 | Generation 41/5000 - Average Score: 73.94399999999999 - Best Score: 440 \n",
      "Saving population at generation 41\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def main():\n",
    "    env = Game2048Env()\n",
    "\n",
    "    generations = 5000\n",
    "    games_per_player = 10\n",
    "    max_steps = 10000\n",
    "    hidden_layers = [2048, 2048, 2048]\n",
    "\n",
    "    optimizer = EvolutionaryOptimizer(\n",
    "        population_size=100,\n",
    "        elite_size=50,\n",
    "        mutation_rate=0.05,\n",
    "        mutation_strength=0.1,\n",
    "        hidden_layers=hidden_layers\n",
    "    )\n",
    "\n",
    "    avg_scores = []\n",
    "\n",
    "    loop_start_time = time.time()\n",
    "\n",
    "    # Create directory for saving networks\n",
    "    import os\n",
    "    folder = f\"networks/{'_'.join(str(x) for x in hidden_layers)}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    best_score = 0\n",
    "\n",
    "    for gen in range(generations):\n",
    "        (population, avg_score, best_score) = optimizer.run_generation(env, games_per_player, max_steps)\n",
    "        avg_scores.append(avg_score)\n",
    "        elapsed_time = time.time() - loop_start_time\n",
    "        average_time_per_iteration = elapsed_time / (gen + 1)\n",
    "        duration = str(timedelta(seconds=(average_time_per_iteration * (generations - gen + 1))))\n",
    "\n",
    "        # Save best network every 10 generations\n",
    "        if (gen % 10 == 0) or (gen == generations - 1):\n",
    "            print(f\"⏳ {duration} | Generation {gen+1}/{generations} - Average Score: {avg_score} - Best Score: {best_score} \")\n",
    "            print(f\"Saving population at generation {gen+1}\")\n",
    "            save_population(population, f'{folder}/population_gen_{gen+1}.pkl')\n",
    "\n",
    "\n",
    "    # evaluated = optimizer.evaluate(env, 10)\n",
    "    # best_network, best_score = max(evaluated, key=lambda x: x[1])\n",
    "\n",
    "    # Plot average scores over generations\n",
    "    plt.plot(range(1, generations + 1), avg_scores)\n",
    "    plt.xlabel('Generation')\n",
    "    plt.ylabel('Average Score')\n",
    "    plt.title('Evolution of Average Score over Generations')\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-2048",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
